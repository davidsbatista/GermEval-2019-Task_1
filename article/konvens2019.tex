%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{url}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{COMTRAVO-DS team at GermEval 2019 Task 1 on hierarchical classification of blurbs}

\author{David S. Batista \\
  Comtravo GmbH \\
  {\tt david.batista@comtravo.com} \\\And
  Matti Lyra \\
  Comtravo GmbH \\
  {\tt matti.lyra@comtravo.com} \\}


\date{}

\begin{document}
\maketitle

\begin{abstract}
We present two systems developed by the Comtravo Data Science team for the GermEval'19 Task 1
on hierarchical classification of blurbs. The challenge is a document classification
task where the hierarchical structure of each document needs to be captured. Our
systems achieved the 13th place out of 19 submissions for Sub-Task A and the 11th
place out of 19 submissions for Sub-Task B. We describe in detail these two
systems pointing out the advantages and disadvantages of each as well as laying
out future research directions.
\end{abstract}




\section{Introduction}

This paper describes the approach taken by the Comtravo Data Science team
for the GermEval'19 Task 1. The task aimed at developing systems to tackle the
task of multi-label hierarchical classification of text.

Several real-world classification problems are naturally cast within a hierarchy,
where the labels to be predicted are organized in an hierarchy. Typically the
hierarchies form a tree, several trees (a forest) or a directed acyclic graph.

Examples of hierarchical document categorization are for instance categorizing
news articles into an hierarchy of categories~\cite{Lewis:2004:RNB:1005332.1005345},
a web page into a web directory structure, Wikipedia articles into the
Wikipedia taxonomy~\cite{PartalasKBAPGAA15}, or in biomedical literature, for
instance, the assignment of Medical Subject Headings to PubMed
abstracts~\cite{lipscomb2000medical}.

We developed two distinct approaches, one based on a local classifier strategy,
where different classifiers are trained according to the hierarchical structure of
the label space, another approach uses a single classifier which tries to naively
predict the entire label hierarchy for each sample.

This paper is organized as follows, in Section~\ref{task} we describe the task in
detail and give a statistical description of the provided dataset. In
Section~\ref{hierarchical-clf} we briefly describe some of the proposed approaches in
the literature for hierarchical document classification. In Section~\ref{system}
we describe our approaches to tackle both sub-tasks. Section~\ref{section_experiments}
details the our experimental setup and results. Finally in Section~\ref{future} we
outline some ideas for future work.



\section{Task}\label{task}

The GermEval 2019 Task 1 on hierarchical classification of blurbs involved the
classification of german language books into genres given a book's blurb i.e., a
short textual description of the book and related meta-data. The competition
contained two sub-tasks:

\begin{itemize}

\item Sub-Task A: classify German books into one or multiple general
genres, a non-hierarchical multi-label classification task with a total of 8 classes.

\item Sub-Task B: targets hierarchical multi-label classification into multiple
writing genres. In addition to the general genres from Sub-Task A, any number of
sub-genres of increasing specificity can be assigned to a book.

\end{itemize}


\subsection{Dataset}

The dataset made available for these tasks contains 3 label levels organized in a hierarchy;
any book can be can associated with more than one label at any given level of the hierarchy.
In the hierarchy every child-label has exactly one parent-label.

The hierarchy contains a total of 343 distinct classes, 3 datasets were
provided: 14 548 samples available for training, 2 079 for development and
4 157 for testing. Tables~\ref{quantitivy-analysis-train}, \ref{quantitivy-analysis-dev},
\ref{quantitivy-analysis-test}  contain a detailed description of
the datasets provided i.e.: training, development and test, respectively. One can
see that in terms of tokens and sentences the 3 datasets are aligned, and also
between training and development in terms of labels per blurb, and labels per blurb
per level.

\begin{table}
\small
\begin{center}
\begin{tabular}{|l|r|}
\hline

\multicolumn{2}{ |c| }{\textbf{Training set}}                   \\
\hline
Avg. length of blurb (tokens)              &  96.78             \\
Std. deviation $\sigma$ (tokens)           &  39.63             \\
Avg. length of blurb (sentences)           &  6.55              \\
Std. deviation $\sigma$ (sentences)        &  2.76              \\
\hline
Nr. unique tokens original                 &  114 903           \\
Nr. unique tokens lowercase                &  107 998           \\
\hline
Total number of genres                     &  343               \\
Possible genres per level (1;2;3)          &  8; 93; 242        \\
Avg. genres per blurb                      &  3.1               \\
Std. deviation $\sigma$                    &  1.36              \\
Avg. genres per blurb at level (1;2;3)     &  1.06; 1.34; 0.69  \\
Std. deviation $\sigma$                    &  0.27; 0.76; 0.79  \\
\hline
Avg. blurb per co-occurrence               &  6.48              \\
Co-Occurrence std. deviation               & 35.90              \\
\hline
Nr. samples with leaf nodes at:            &                    \\
 - Level 1                                 & 1.9\% (311)        \\
 - Level 2                                 & 44,6\% (7.422)     \\
 - Level 3                                 & 53,5\% (8.894)     \\
 \hline
 Total number of samples                   & 14 548             \\
\hline
\end{tabular}
\end{center}
\caption{\label{quantitivy-analysis-train}Quantitative analysis of the training dataset.}
\end{table}

\begin{table}
\small
\begin{center}
\begin{tabular}{|l|r|}
\hline
\multicolumn{2}{ |c| }{\textbf{Development set}}              \\
\hline
Avg. length of blurb (tokens)              &   98.71        \\
Std. deviation $\sigma$ (tokens)           &   46.29        \\
Avg. length of blurb (sentences)           &   6.68         \\
Std. deviation $\sigma$ (sentences)        &   3.80         \\
\hline
Nr. unique tokens original                 &  33 599        \\
Nr. unique tokens lowercase                &  31 818        \\
\hline
Total number of genres                     &   343             \\
Possible genres per level (1;2;3)          &   8; 93; 242      \\
Avg. genres per blurb                      &   3.1             \\
Std. deviation $\sigma$                    &   1.39            \\
Avg. genres per blurb at level (1;2;3)     &   1.07;1.35;0.69  \\
Std. deviation $\sigma$                    &   0.27;0.80;0.79  \\
\hline
Avg. blurb per co-occurrence               &   3.08            \\
Co-Occurrence std. deviation               &   8.19            \\
\hline
Nr. samples with leaf nodes at:            &                   \\
 - Level 1                                 &  1.6\% (34 )      \\
 - Level 2                                 &  44.8\% (932 )    \\
 - Level 3                                 &  53.6\% (1113)    \\
\hline
Total number of samples                    &  2 079            \\
\hline
\end{tabular}
\end{center}
\caption{\label{quantitivy-analysis-dev}Quantitative analysis of the development dataset.}
\end{table}


\begin{table}
\small
\begin{center}
\begin{tabular}{|l|r|}
\hline
\multicolumn{2}{ |c| }{\textbf{Test set}}                       \\
\hline
Avg. length of blurb (tokens)              &  96.91             \\
Std. deviation $\sigma$ (tokens)           &  39.83             \\
Avg. length of blurb (sentences)           &  6.55              \\
Std. deviation $\sigma$ (sentences)        &  2.62              \\
\hline
Total number of samples                    &  4 157             \\
\hline
\end{tabular}
\end{center}
\caption{\label{quantitivy-analysis-test}Quantitative analysis of the test dataset.}
\end{table}

\section{Hierarchical Document Classification}\label{hierarchical-clf}

There are have been different strategies to approach the problem of hierarchical
classifying a document~\cite{Silla:2011:SHC:1937796.1937884,
Wehrmann:2017:HMC:3019612.3019664, KowsariBHMGB17}.
Within the context of GermEval'19 Task 1 we explored two main strategies: a local
classifier and a global classifier.

\subsection{Local Classifier}

The local classifier strategy is one way to approach the hierarchical document
classification task and it was first proposed, to the best of our knowledge, in
the seminal work of~\citet{Koller:1997:HCD:645526.657130}, it is also sometimes
referred to as top down approach in the literature.

There are different approaches, based on the idea of a local classifier,
depending on how they use the local information and devise a strategy to build
several classifiers.


\subsubsection{A classifier per node}

The \textit{local classifier per node approach} consists of training one binary
classifier in a one-versus-rest scenario for each node in the hierarchy, where
each label in the hierarchy is a node. Normally, the negative training data is
taken from the same level in the label hierarchy as the positive data.

During prediction a top-down strategy is applied, the output of each binary
classifier is a prediction of whether or not a given test sample belongs to the
classifier’s predicted class. This approach is naturally multi-label since it is
possible to predict multiple labels at each level of the hierarchy.

This approach, however, is prone to label inconsistency. Consider a document
that has, for the first level, labels 1, 2 and 3, and, for the second level,
labels 1.1, 1.2. Since the classifiers for nodes 1 and 1.1 are independently
trained, it is possible to classify a sample as having labels 1.2 and 1.1 but
not the parent label 1. This approach should, therefore, be complemented by a
post-processing method that tries to correct the label inconsistency.


\subsubsection{A classifier per parent node}

In \textit{a classifier per parent node} approach, a multi-class classifier,
possibly also multi-label, is trained for each parent node in the label
hierarchy. The classifier is trained to classify the probability of a given
sample belonging to each of the parent's child nodes. A parent node is in this
case every label in the hierarchy-tree which has one or more child labels.

Given a test sample, first the top-level classifier is applied, then for every
top-level predicted label (e.g., class 2 and 3) its child classifiers, e.g.: a
classifier trained to predict the 2.x labels and another for 3.x labels, and so
on until the last level is reached.

Note that the sub-classifiers are only trained with the children of each
respective parent label, therefore this approach avoids the label inconsistency
problem and respects the constrains of class-membership defined by the label
hierarchy.


\subsubsection{A classifier per level}

The \textit{local classifier per level} approach consists of training one
multi-class, and possibly also multi-label, classifier for each level of the label
hierarchy. When a new test sample is presented the output of the classifiers
from each level is used as the final classification.

This approach, however, is prone to label inconsistency, as different classifiers
are trained for each level of the hierarchy and should, therefore, also be
complemented by a post-processing method to correct the prediction inconsistency.

One common error for all local classifier strategies the utilize the top-down
class-prediction approach is the propagation of errors down the hierarchy.



\subsection{Global Classifier}

Another type of strategy is to learn a classifier than can globally learn to
output the predictions for each level in the hierarchical structure. This is
done by flattening the whole hierarchical structure.

Having only a single classifier, although easier to tune, it can turn the
hierarchical classification into a much harder problem, specially having a
sparse label space with an order of magnitude of $10^2$, but the labels
co-occurrence can be a good guiding heuristic for a statistical model to infer
the hierarchical label structure associated with a given sample.

\section{Systems Developed}\label{system}

We developed two systems implementing the following approaches:

\begin{description}
  \item[Local Classifier:] a classifier per parent node using
  different types of classifiers;
  \item[Global Classifier:] a single classifier relying on the hypothesis
  to explore the labels co-occurrence;
\end{description}

\subsection{Local Classifier}

We employed a classifier per parent node approach, which has the advantage of
not being prone to label inconsistency errors. We need to train classifiers for
each parent node. For Level 3 we don't need to train any classifier since it
contains only leaf nodes, plus some nodes on Level 2 are already leaf nodes.

According to Table~\ref{quantitivy-analysis-train} the Level 1 has 8 possible
labels, which means that the parent node of the first level (i.e, the Root Node)
needs to be trained in a multi-label fashion and predict over 8 classes. Each of
these 8 classes represents a parent node of some child classes on the
next level of the hierarchy. So for Level 1 we need to train eight multi-label
classifiers where the labels are the child`s of each parent in the root level.
Finally, for Level 2, we train 42 classifiers only, since according to the
hierarchy-tree some labels in this level are already leaf nodes, and only 42
labels have then child labels. So, in total we trained 51 classifiers
distributed by different levels as described in Table~\ref{parent-per-node-classifiers}.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|}
\hline\centering\textbf{Level}  &  \textbf{Nr. Parent Nodes}    \\
\hline
Root Node         &  1       \\
Level 1           &  8       \\
Level 2           &  42      \\
\hline
Total Classifiers & 51       \\
\hline
\end{tabular}
\end{center}
\caption{\label{parent-per-node-classifiers}Number of parent nodes per level in
the hierarchy.}
\end{table}

As stated before one of the advantages of this approach is the that it always
produces a label structure that is enforced by the hierarchy-tree, but it is
prone to error propagation from the top levels further down the tree.


\subsection{Global Classifier}

The global classifier needs to be a multi-label classifier targeting a label
space with a total of 343 classes. One of the advantages is that there is only
one single classifier to tune and explore, but on the other and it has a high
and sparse label space, plus one needs to employ some post-processing cleaning
to enforce the hierarchical structure.

\section{Experiments and Results}\label{section_experiments}

In this section we describe the experimental setup and results for the two
devised strategies for tackling both sub-tasks.

\subsection{Results on the Development Set}\label{experiments}

During the development phase we only had the labels for training, so we
randomly split the training dataset into two-subsets of 70\% and 30\% for
training and parameter tuning, respectively. Then, train on the whole
dataset with the best parameters and using the model to generate predictions on
the development dataset. This approach was mainly to have a working framework
for experiments and submit valid results.

During the test phase the labels for the development set were made available
and we could use them to tune the classifiers. We used 3-fold cross-validation
to perform parameter search using the training dataset. The parameter
configuration which yielded the best results was then used to train
the classifiers over the complete training dataset, and the classifier is then
evaluated against the development set. Results reported in this section
are all in regard to the development set.

\subsubsection{Pre-processing}

For representing a book we concatenated the book's title with book's textual
description. We explored two tokenization schemas, one tokenizes the blurbs into
sentences, and then from sentences into tokens, considering the title of the
book as a sentence, this tokenization strategy was based on the german sentence
tokenizer, and the \textit{word\_punkt\_tokenizer} from NLTLK 3.4.1~\cite{Bird:2009:NLP:1717171},
and considered alphanumeric tokens only. The other approach was based on a
simple regular expression: \verb|(?u)\b\w\w+\b|. We also experimented lower casing
and removing stop-words. After running a few experiments and comparing some
initial results we opted for the regular expression for tokenization, lower case
token representations and removal of stop-words. For the neural networks the
padding was done to match the size of the longest document in the dataset.


\subsubsection{Prediction threshold}\label{threshold}

We set the prediction threshold to 0.5, so any label with a predicted probability
above 0.5 is selected. We noticed that for some samples no labels were
being selected, as all labels had predicted probability scores lower than 0.5.
To tackle this problem, for a given sample, if no predictions were done we lowered
the threshold to 0.4, if still no label predictions are done, we lowered again
the threshold to 0.3.


\subsubsection{Models implementation}

For all the neural network models we used pre-trained embeddings, specifically
the public available German fastText embeddings trained on Wikipedia, of
dimension 300 and obtained using the skip-gram model as described
in~\citet{bojanowski-etal-2017-enriching}, the embeddings are fine-tuned
during learning and out of vocabulary words are randomly initialized.

The training was done with the Adam optimizer~\cite{journals/corr/KingmaB14}
using binary cross-entropy as a loss function and 30\% of the training
dataset for validation. Unless stated otherwise training was performed with
mini-batch sizes of 16 for 10 epochs. All the neural network models were
implemented in Keras 2.2.4 with Tensorflow 1.13.1 backend. The Logistic
Regression classifier was based on the scikit-learn
0.21.1~\cite{Pedregosa:2011:SML:1953048.2078195}.

All the code used for this experiments is available
on-line~\footnote{\url{https://github.com/davidsbatista/GermEval-2019-Task_1}}.


\subsubsection{Local classifier per node: Root Node}

We explored different classifiers for the Root Node and for Level 1 and Level 2
we selected a Convolutional Neural Network (CNN).

We briefly describe the models used for the Root Node and the parameters explored,
in bold we have the parameters that yielded the best scores:

\begin{description}

\item[\textbf{Logit (TF-IDF)}:] a logistic regression classifier with TF-IDF
weighted vectors in a one-versus-rest scenario varying the following parameters:

\begin{itemize}
\item n-grams: 1, \textbf{2}, 3;
\item class weight: \textbf{balanced}, not-balanced;
\item norm: l1, \textbf{l2};
\item regularization $C$: 0.1, 10, 100, \textbf{300};
\end{itemize}

Training was performed with Stochastic Average Gradient for a maximum of 5 000
iterations.

\item[\textbf{CNN}]\cite{kim-2014-convolutional}: for sentence classification
with rectified linear units in the activation functions of the 1D convolutions,
and varying:

\begin{itemize}
\item filter windows: (1), \textbf{(1, 2)}, (1, 2, 3);
\item feature maps: 256, \textbf{300};
\end{itemize}

\item[\textbf{LSTM}]\cite{Hochreiter:1997:LSM:1246443.1246450}: recursively reads
each token in the text updating it's internal state and using the last state to
represent the document, with a dropout layer of rate 0.1 between the LSTM's last
state and the final sigmoid layer.

\begin{itemize}
\item single LSTM vs \textbf{bi-directional LSTM};
\item hidden units: 32, 64, \textbf{128};
\end{itemize}

\item[\textbf{Bag-of-Tricks}]\cite{joulin-etal-2017-bag}: token embeddings
representations are averaged into a single vector representation, which is fed
to a sigmoid classifier, we varied the following parameters:

\begin{itemize}
\item n-grams: \textbf{2}, 3, 4, 5;
\item top-k most frequent tokens: \textbf{100k}, 90k, 80k;
\end{itemize}

\end{description}

Table~\ref{subtask_a_devset-results} shows the results for different classifiers
when trained with the best parameters on the training set and evaluated against
the development set.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Method}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Logit (TF-IDF) & 0.8211 & \textbf{0.8359} & \textbf{0.8284} \\
 CNN            & \textbf{0.8542} & 0.7879 & 0.8197 \\
 bi-LSTM        & 0.8062 & 0.7987 & 0.8024 \\
 Bag-of-Tricks  & 0.3787 & 0.6717 & 0.4843 \\
\hline
\end{tabular}
\end{center}
\caption{\label{subtask_a_devset-results} Results for different classifiers on
         the Sub-Task A on the development set.}
\end{table}

The Logistic Regression classifier achieved the best results
although the CNN classifier  had a similar F\textsubscript{1} score, essentially
by trading recall for precision.


\subsubsection{Local classifier per node: Level 1 and 2}

For Level 1 and 2 in the hierarchy tree we trained a total of 50 classifiers
for each parent node, 8 for Level 1 and 42 for Level 2. All these classifiers
were based on the CNN model.

We explored some parameters configurations by varying the filter windows size and
the filter maps size, Table~\ref{subtask_b_parameters_local} shows a subset of the different
configuration parameters tried which yielded some of the best results. All these
classifiers were training with mini-batch size of 16 for 5 epochs.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{}  & \textbf{Filter Windows} &  \textbf{Filter Maps} \\
\hline
\textbf{Conf\textsubscript{1}} & \textbf{1,2}       & \textbf{300} \\
Conf\textsubscript{2} & 1,2,3     & 200 \\
Conf\textsubscript{3} & 1,2,3,5,7 & 300 \\
Conf\textsubscript{4} & 3,5,6,10  & 256 \\
\hline
\end{tabular}
\end{center}
\caption{\label{subtask_b_parameters_local} Different configuration parameters
for the CNN classifiers for Level 1 and 2.}
\end{table}

In these experiments we used the Root Node classifier which yielded the best
results, i.e. the logistic regression, to predict the labels for the root level,
and experiment with different parameters for the Levels 1 and 2.
Table~\ref{level-1-2} shows the results for Sub-Task B for configurations of
parameters in Table~\ref{subtask_b_parameters_local}.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
Conf\textsubscript{1} & 0.7151 & \textbf{0.5330} & \textbf{0.6108} \\
Conf\textsubscript{2} & 0.7144 & 0.5303 & 0.6087 \\
Conf\textsubscript{3} & 0.7219 & 0.5235 & 0.6069 \\
Conf\textsubscript{4} & \textbf{0.7274} & 0.5085 & 0.5986 \\
\hline
\end{tabular}
\end{center}
\caption{\label{level-1-2} Results for Sub-Task B for different configurations
of the CNN-based classifier for Levels 1 and 2 of the hierarchy, using the best
classifier for the Root Level from Table~\ref{subtask_b_parameters_local}.}
\end{table}






\subsubsection{Global classifier}

The global classifier uses a flattened hierarchy and learns how to predict a
vector of 343 dimensions.

One advantage of this approach in contrast to the local one is the we need only
to tune a single classifier, and both sub-tasks can also be tackled with this
single classifier.

Our initial idea was to use a neural network architecture and leverage on the
labels co-occurrence by initializing a weight matrix, but due to time constrains
this was not possible to take to the end. Also, due to time constrains we did not
employ a post-processing step.

Nevertheless, we still applied a CNN architecture and explored different
configuration parameters, varying the filter windows and the filter maps.

We used the same tokenization schema as with the Local classifier, as well
as the same pre-trained word embeddings, which are fine-tuned during training.

The training was done with the Adam optimizer~\cite{journals/corr/KingmaB14}
using binary cross-entropy as a loss function, with mini-batch sizes of 128 for
250 epochs and using 30\% of the training dataset for validation.

We also used the same threshold filtering strategy as described in Section~\ref{threshold}.

Table~\ref{subtask_b_parameters_global} presents some of the configurations of
parameters used in the experiments and Table~\ref{results_global_b} the
corresponding results for the same configurations for both sub-tasks.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{}  & \textbf{Filter Windows} &  \textbf{Filter Maps} \\
\hline
Conf\textsubscript{1} &           1, 2, 3  &  300 \\
Conf\textsubscript{2} &        3, 5, 7, 10 &  256 \\
\textbf{Conf\textsubscript{3}} &  \textbf{1, 2, 3, 5, 7, 10} &  \textbf{256} \\
\hline
\end{tabular}
\end{center}
\caption{\label{subtask_b_parameters_global} Different configuration parameters
for the CNN classifiers for the global classifier.}
\end{table}

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
\multicolumn{4}{ |l| }{\textbf{Conf\textsubscript{1}}}              \\
\hline
 Sub-Task A   & 0.7163  & 0.7484 & 0.7320 \\
 Sub-Task B   & 0.5257  & 0.4603 & 0.4909 \\
\hline
\multicolumn{4}{ |l| }{\textbf{Conf\textsubscript{2}}}              \\
\hline
 Sub-Task A   & 0.7353  & \textbf{0.7686} & 0.7516 \\
 Sub-Task B   & 0.5470  & 0.4717 & 0.5066 \\
\hline
\multicolumn{4}{ |l| }{\textbf{Conf\textsubscript{3}}}              \\
\hline
 Sub-Task A   & \textbf{0.8389}  & 0.7659 & \textbf{0.8008} \\
 Sub-Task B   & \textbf{0.6733}  & \textbf{0.5032} & \textbf{0.5760} \\
\hline
\end{tabular}
\end{center}
\caption{\label{results_global_b} Results for both sub-tasks using the
         configuration parameters from Table~\ref{subtask_b_parameters_global}.}
\end{table}







\subsection{Test Results}

We applied the classifiers described in the Section~\ref{section_experiments} with the
parameters that yielded the best results on the development dataset, by training
on all available data (i.e., training + development sets) and applied them
on the test dataset, therefore generating two submissions for each sub-task.

\subsubsection{Parent Per Node}

With the one parent per node strategy classifier, we achieved the 13th best place
on Sub-Task A, and the 11th best place on Sub-Task B, out of a total of 19 submissions.
Results for the detailed evaluation metrics are described in Table~\ref{local_devset-results}.

This classifier achieved the 9th best recall and the 15th best precision for
Sub-Task A, and the 8th best recall and the 14th best precision for Sub-Task B.
This results were achieved with a support of 0.943, meaning that for roughly 5\%
of the samples the classifier did not managed to produce any predictions.


\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Task}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Sub-Task A   &  0.8144 & 0.8255 & 0.8199 \\
 Sub-Task B   &  0.7042 & 0.5274 & 0.6031 \\
\hline
\end{tabular}
\end{center}
\caption{\label{local_devset-results} Best achieved results on the development
          set for both Sub-Tasks A and B with the parent per node classifier.}
\end{table}





\subsubsection{Global}

With the global classifier strategy we achieved the 17th best place on Sub-Task A,
and the 13th best place on Sub-Task B. Detailed results are presented in
Table~\ref{global_devset-results}.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Task}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Sub-Task A   &  0.7761 & 0.7839 & 0.7839 \\
 Sub-Task B   &  0.5672 & 0.5185 & 0.5418 \\
\hline
\end{tabular}
\end{center}
\caption{\label{global_devset-results} Best achieved results on the development
          set for both Sub-Tasks A and B with the global classifier.}
\end{table}

This classifier achieved the 15th best recall and the 17th best precision
for Sub-Task A, and the 9th best recall and the 18th best precision for Sub-Task B.
The support was 0.9986, meaning that for roughly 1\% of the samples the classifier
did not produced any predictions. The hierarchy consistency is of 0.9363, reflecting
the lack of a post-processing step to enforce the hierarchical structure,
which would be 1.0 if employed.

\section{Future Work}\label{future}

We had planned to explore different features and carry more experiments but time
constraints for the submission of the test results did not allowed us to experiment
all that was planned.

% Global Classifier
One crucial aspect in the global classifier is a post-processing step to make
sure that the labels output is in-line with the hierarchy-tree constrains. The
global classifier could also be improved by initializing a weight matrix
based on label co-occurrence. \citet{kurata-etal-2016-improved} proposed a neural
network initialization method to treat some of the neurons in the final hidden
layer as dedicated neurons for each pattern of label co-occurrence. These dedicated
neurons are initialized to connect to the corresponding co-occurring labels with
stronger weights than to others representing non co-occurring labels. \citet{baker-korhonen-2017-initializing}
applied this idea in the biomedical domain and to a much more compact hierarchy
than the one presented in this paper.


% Local Classifier
In the local classifier strategy for Level 1 and 2 we use the same architecture
for all classifiers and tuned them in the same way, the type of architecture and
tuning process could be made dependent on the numbers of samples available to
train and level in the hierarchical tree.

A few more features could have been explored, for instance the author's name
and the release date of the book. The padding of the documents representation
for the neural network could be set to the average since of the documents,
instead of the longest one.

The values for the prediction threshold were selected in an ad-hoc fashion, these
could also be properly set, through a set of experiments.



\bibliography{konvens2019.bib}
\bibliographystyle{acl_natbib}

\end{document}
