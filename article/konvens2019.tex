%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{url}
%\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{COMTRAVO-DS team at GermEval 2019 Task 1 on hierarchical classification of blurbs}

\author{David S. Batista \\
  Comtravo GmbH \\
  {\tt david.batista@comtravo.com} \\\And
  Matti Lyra \\
  Comtravo GmbH \\
  {\tt matti.lyra@comtravo.com} \\}


\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a detailed description of the systems used for the GermEval'19 Task 1
on hierarchical classification of blurbs. The challenges in this type of document
classification are related with capturing the correct hierarchical structure of
each document to label.
We developed two systems, with which we generated two different submissions
achieving the 13th place out of 19th submissions for Sub-Task A and 11th place
out of 19th submissions for Sub-Task B. We describe in detailed these two systems
pointing out the advantages and advantages of each and lay down some foundations
for some possibly future work on how to improve this results.
\end{abstract}




\section{Introduction}

This paper describes the approach taken for the GermEval'19 task 1 taken by the
Comtravo data-science team. The task aimed at developing systems to tackle the
task of multi-label hierarchical classification of text. We took two distinct
approaches, one based on a local classifier strategy, where different classifiers
are trained according to the hierarchical structure of the label space. Another
approach uses a single classifier which tries to naively predict the label
hierarchy for each sample.

This paper is organized as follows, in Section~\ref{task} we describe task in
detail and give a description of the dataset provided. In Section~\ref{hierarchical-clf}
we describe some of the proposed approaches in the literature to performer
hierarchical document classification. In Section~\ref{system} we describe the
system we build to tackle the task and the approaches taken. Section~\ref{experiments}
details the experiments done and the results, finally in Section~\ref{future} we
outline some future work and ideas.



\section{Task}\label{task}

The GermEval 2010 Shared Task on hierarchical classification of blurbs challenges
involved the classification of books into genres given a book's blurb i.e., a
short textual description of the book. The competition contained two sub-tasks:

\begin{itemize}

\item Sub-Task A: classify German books into one or multiple most general writing
genres. Therefore, it can be considered a multi-label classification task. In
total, there are 8 classes that can be assigned to a book.

\item Sub-Task B: targets hierarchical multi-label classification into multiple
writing genres. In addition to the very general writing genres, additional
genres of different specificity can be assigned to a book.

\end{itemize}


\subsection{Dataset}

The dataset made available for this task contains 3 levels and it's considered
a hierarchically multi-label since any example can be can assigned to more than
one class at any given level of the hierarchy.

In the hierarchy every child-label has exactly one parent-label and to any given
sample multiple labels can be assigned, so one can think of this as an hierarchy
made of several trees, a forest.

The hierarchy contains a total of 343 classes distinct classes, and 3 datasets
were provided: 14 548 samples available for training, 2 079 for development and
4 157 for testing.

The Tables~\ref{quantitivy-analysis-train}, \ref{quantitivy-analysis-dev},
\ref{quantitivy-analysis-test}  contain a detailed statistical description of
the datasets provided i.e.: training, development and test. One can see that in
terms of tokens and sentence the 3 datasets are well aligned.

\begin{table}
\small
\begin{center}
\begin{tabular}{|l|r|}
\hline\centering\textbf{Training set}  &                        \\
\hline
Avg. lenght of blurb (tokens)              &  96.78             \\
Std. deviation $\sigma$ (tokens)           &  39.63             \\
Avg. lenght of blurb (sentences)           &  6.55              \\
Std. deviation $\sigma$ (sentences)        &  2.76              \\
\hline
Nr. unique tokens original                 &  114 903           \\
Nr. unique tokens lowercase                &  107 998           \\
\hline
Total number of genres                     &  343               \\
Possible genres per level (1;2;3)          &  8; 93; 242        \\
Avg. genres per per blurb                  &  3.1               \\
Std. deviation $\sigma$                    &  1.36              \\
Avg. genres per blurb at level (1;2;3)     &  1.06; 1.34; 0.69  \\
Std. deviation $\sigma$                    &  0.27; 0.76; 0.79  \\
\hline
Avg. blurb per co-occurrence               &  6.48              \\
Co-Occurrence std. deviation               & 35.90              \\
\hline
Nr. samples with leaf nodes at:            &                    \\
 - Level 1                                 & 1.9\% (311)        \\
 - Level 2                                 & 44,6\% (7.422)     \\
 - Level 3                                 & 53,5\% (8.894)     \\
 \hline
 Total Nr. of samples                      & 14 548             \\
\hline
\end{tabular}
\end{center}
\caption{\label{quantitivy-analysis-train}Quantitative analysis of the training dataset.}
\end{table}

\begin{table}
\small
\begin{center}
\begin{tabular}{|l|r|}
\hline\centering\textbf{Development set}  &             \\
\hline
Avg. lenght of blurb (tokens)              &   98.71        \\
Std. deviation $\sigma$ (tokens)           &   46.29        \\
Avg. lenght of blurb (sentences)           &   6.68         \\
Std. deviation $\sigma$ (sentences)        &   3.80         \\
\hline
Nr. unique tokens original                 &  33 599        \\
Nr. unique tokens lowercase                &  31 818        \\
\hline
Total number of genres                     &   343             \\
Possible genres per level (1;2;3)          &   8; 93; 242      \\
Avg. genres per per blurb                  &   3.1             \\
Std. deviation $\sigma$                    &   1.39            \\
Avg. genres per blurb at level (1;2;3)     &   1.07;1.35;0.69  \\
Std. deviation $\sigma$                    &   0.27;0.80;0.79  \\
\hline
Avg. blurb per co-occurrence               &   3.08            \\
Co-Occurrence std. deviation               &   8.19            \\
\hline
Nr. samples with leaf nodes at:            &                   \\
 - Level 1                                 &  1.6\% (34 )      \\
 - Level 2                                 &  44.8\% (932 )    \\
 - Level 3                                 &  53.6\% (1113)    \\
\hline
Total Nr. of samples                       &  2 079            \\
\hline
\end{tabular}
\end{center}
\caption{\label{quantitivy-analysis-dev}Quantitative analysis of the development dataset.}
\end{table}


\begin{table}
\small
\begin{center}
\begin{tabular}{|l|r|}
\hline\centering\textbf{Test set}  &         \\
\hline
Avg. lenght of blurb (tokens)              &  96.91             \\
Std. deviation $\sigma$ (tokens)           &  39.83             \\
Avg. lenght of blurb (sentences)           &  6.55              \\
Std. deviation $\sigma$ (sentences)        &  2.62              \\
\hline
Total Nr. of samples                       &  4 157             \\
\hline
\end{tabular}
\end{center}
\caption{\label{quantitivy-analysis-test}Quantitative analysis of the test dataset.}
\end{table}

\section{Hierarchical Document Classification}\label{hierarchical-clf}

Several real-world classification problems are naturally cast within a hierarchy,
where the labels to be predicted are organized in an hierarchy, typically as
tree, several trees (i.e., a forest) or a DAG (Direct Acyclic Graph).

Some examples of hierarchical document categorization are for instance categorizing
a news article~\cite{Lewis:2004:RNB:1005332.1005345}, a web page into a web
directory or Wikipedia articles~\cite{PartalasKBAPGAA15}.

There are different strategies to approach this problem and within the context
of GermEval'19 Task 1 we explored two strategies: local classifier and a
global classifier.


\subsection{Local Classifier}

The local classifier strategy is one way to approach the hierarchical document classification task
and it was first proposed, to the best of our knowledge, in the seminal work of Koller and
Sahami~\shortcite{Koller:1997:HCD:645526.657130}, it is also sometimes referred to as top-down
approach in the literature.

There are different approaches, based on the idea of a local classifier, depending on how they use
the local information and devise a strategy to build several classifiers~\cite{Silla:2011:SHC:1937796.1937884}.


\subsubsection{A classifier per node} % Local Classifier Per Node Approach
The \textit{local classifier per node approach} consists in training one binary classifier for each
node in the hierarchy tree, where a node is every possible label in the hierarchy tree.

For instance, having classes 1, 2 and 3 in the first level, one classifier can be trained with
samples belonging to class 1 as positive and classes 2 and 3 as negative, a similar approach is then
followed for the the other classes and lower levels in the hierarchy. There are different strategies
on how to select the positive and negative classes (see~\cite{}).

During prediction a top-down strategy is applied, the output of each binary classifier will be a
prediction of whether or not a given test sample belongs to the classifierâ€™s predicted class. This
approach is naturally multi-label since it is possible to predict multiple labels per class level.

This approach however is prone to class-membership inconsistency, consider having again in the first
level classes 1, 2 and 3, and in the second level, for instance, classes 1.1, 1.2, since the
classifiers for nodes 1 and 1.1 are independently trained, one can classify a sample as belonging to
classes 1.2 and 1.1 but not to class 1. This approach should also be complemented by a
post-processing method that tries to correct the prediction inconsistency.


\subsubsection{A classifier per parent node} % Local Classifier Per Parent Node Approach
In \textit{a per parent node approach}, for each parent node in the hierarchy tree, a multi-class
classifier is trained to classify the probability of a given sample belonging to each of the parent's
child nodes. A parent node is in this case every label in the hierarchy-tree which has one or more
child labels.

Given a test sample, first the top-level classifier is applied, then for every top-level
predicted class (e.g., class 2 and 3) we apply it's child classifiers, e.g.: a classifier
trained to predicted every 2.x classes and another for 3.x classes, and so one until we reach the
last level.

Note that the sub-classifiers were trained only with the children of classes 2 and 3, therefore
this approach avoids the problem of making inconsistent predictions and respects the constrains
of class-membership defined by the hierarchy-tree.


\subsubsection{A classifier per level} % Local Classifier Per Level Approach
The \textit{local classifier per level} approach consists of training one multi-class classifier
for each level of the hierarchy-tree. When a new test sample is presented we get the output of the
classifiers from each level and use this information as the final classification.

This approach however is prone to class-membership inconsistency, since different classifiers are
trained for each level of the hierarchy. For instance, it's possible to have has outputs for the
first-level classes 2 and 3, and then classes 1.1 and 1.2 at the second-level, and class 3.1.1
at the third-level, which generates an inconsistent classification. This approach should also
be complemented by a post-processing method that tries to correct the prediction inconsistency.

One common error to all strategies in the local classifier and the top-down class-prediction
approach is the propagation of errors downwards the hierarchy.



\subsection{Global Classifier}

Another type of strategy is to learn a classifier than can globally learn to
output the predictions for each level in the hierarchical structure. This is
done by flattening and hierarchical structure and one idea one can explore is to
leverage on the labels co-occurrence.

Although having only a single classifier, can turn the hierarchical classification
into a much harder problem, specially with a sparse label space and with an order
of magnitude of $10^2$, the labels co-occurrence can be a good guiding heuristic
for a statistical model to infer the hierarchy label structure associated with
a given sample.

\section{Systems Developed}\label{system}

We developed two systems implementing the following approaches: a classifier per
parent node (\textit{local\_clf\_logit\_cnn}), and global classifier relying
on the hypothesis to explore the labels co-occurrence (\textit{global\_clf\_cnn}).

\subsection{Local Classifier}

We employed a parent per parent approach, which has the advantage of not being
prone to hierarchy inconsistency errors. We need to train classifiers for each
parent node, except for the leaf nodes. For the Level 2 we don't need to train
any classifier since it contains only leaf nodes, plus some nodes on Level 1
are already leaf nodes.

According to Table~\ref{quantitivy-analysis-train} the first level has 8 possible
labels, which means that the parent node of the first level (i.e, the Root node)
needs to be trained in a multi-label fashion and predict over 8 classes. Each of
these 8 classes represents a parent node of some child classes on the
second level of the hierarchy. So for Level 1 we need to train eight multi-label
classifiers where the labels are the child`s of each parent in the root level.
Finally, for Level 2, we train 42 classifiers only, since according to the
hierarchy-tree some labels in this level are already leaf nodes, and only 42
labels have then child labels. So, in total we trained 51 classifiers
distributed by different levels as described in Table~\ref{parent-per-node-classifiers}.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|}
\hline\centering\textbf{Level}  &  \textbf{Nr. Parent Nodes}    \\
\hline
Root              &  1       \\
Level 1           &  8       \\
Level 2           &  42      \\
\hline
Total Classifiers & 51       \\
\hline
\end{tabular}
\end{center}
\caption{\label{parent-per-node-classifiers}Number of parent nodes per level in
the hierarchy.}
\end{table}

As stated before one of the advantages of this approach is the that it always
produces a label structure that is enforced by the hierarchy-tree, but it is
prone to error propagation from the top levels further down the tree.


\subsection{Global Classifier}

The global classifier needs to be a multi-label classifier targeting a label
space with a total of 343 classes. One of the advantages is that there is only
one single classifier to tune and explore, but on the other and it has a high
and sparse label space, plus one needs to employ some post-processing cleaning
to enforce the hierarchical structure.

\section{Experiments and Results}\label{experiments}

In this section we describe the experimental setup and results for the two
devised strategies for tackling both subtasks.

\subsection{Results on the Development Set}

During the development phase we only had the labels for the training, so we
randomly split the training dataset into two-subsets of 70\% and 30\% for
training and parameter tuning, training then on the whole dataset and applying
the trained model to the development dataset. This approach was mainly to have
a working framework for experiments.

Then, during the test phase the labels for the development set were made available
and we could use them to tune/test the classifiers. For both strategies we used
3-fold cross fold validation to perform parameter search using the training dataset.
The parameter configuration which yielded the best results was then used to train
the classifiers over the complete training dataset, and the classifier is then
evaluate against the development set. Results reported in this section are all
in regard to the development set.

\subsubsection{Pre-processing}

For representing a book we concatenated the book's title with the textual
description. We explored two tokenization schemas, one tokenizes the blurbs into
sentences, and then from sentences into tokens, considering the title of the
book as a sentence, this based the german sentence tokenizer, and the
\textit{word\_punkt\_tokenizer} from NLTLK 3.4.1~\cite{Bird:2009:NLP:1717171},
and considered alphanumeric tokens only. The other approach was based on
simple regular expression: \verb|(?u)\b\w\w+\b|. We also experimented lower casing
and removing stop-words. After running a few experiments and comparing some
initial results we opted for the regular expression for tokenization, lower case
token representations and removal of stop-words. For the neural networks the
padding was done to match the size of the longest document in the dataset.


\subsubsection{Prediction threshold}

We set the probably prediction threshold at 0.5, so any label prediction probability
above 0.5 is selected. But we noticed that for some predictions no labels were
being selected being correct labels at lower probability scores. For a given
sample, if no predictions are done we lower the threshold to 0.4, if
still no label predictions are done, we lower again the threshold to 0.3.



\subsubsection{Models implementation}

For all the neural network models we used pre-trained embeddings, specifically
the public available German fastText embeddings trained on Wikipedia, of
dimension 300 and obtained using the skip-gram model as described
in~\citet{bojanowski-etal-2017-enriching}, the embeddings which are fine-tuned
during learning and out of vocabulary words are randomly initialized.

The training was done with the Adam optimizer~\cite{journals/corr/KingmaB14}
using binary cross-entropy as a loss function and the 30\% set of the training
dataset for validation. Unless stated otherwise training was performed with
mini-batch sizes of 16 for 10 epochs. All the neural network models were implemented in
Keras 2.2.4 with Tensorflow 1.13.1 backend. The Logistic Regression classifier
was trained based on the scikit-learn 0.21.1~\cite{Pedregosa:2011:SML:1953048.2078195}.

All the code used for this experiments is available
on-line~\footnote{\url{https://github.com/davidsbatista/GermEval-2019-Task_1}}.


\subsubsection{Local classifier per node: root}

We explored different root classifiers and for Level 1 and Level 2 a
selected a Convolutional Neural Network (CNN). For the root level classifier we
explored the following approaches: Logistic Regression with TF-IDF weighted
vectors~\cite{}, CNN~\cite{kim-2014-convolutional},
LSTM~\cite{Hochreiter:1997:LSM:1246443.1246450} and
Bag-of-Tricks~\cite{joulin-etal-2017-bag}.


\begin{description}

\item[\textbf{Logit (TF-IDF)}:] a logistic regression classifier with TF-IDF
weighted vectors in a one-versus-rest scenario varying following parameters:

\begin{itemize}
\item n-grams: 1, 2, 3;
\item class weight: balanced, not-balanced;
\item norm: l1, l2;
\item regularization C: 0.1, 10, 100, 300;
\end{itemize}

Training was performed with Stochastic Average Gradient for a maximum of 50000 iterations.

\item[\textbf{CNN}] for sentence classification with rectified linear units in
the activation functions of the 1D convolutions, and varying:

\begin{itemize}
\item filter windows: 1, 2, 3;
\item feature maps: 256, 300;
\end{itemize}

\item[\textbf{LSTM}]: recursively reads each token in the text updating it's
internal state using the last state to represent the document, with a dropout
layer of rate 0.1 between the LSTM's last state and the final sigmoid layer.

\begin{itemize}
\item single LSTM vs bi-directional LSTM;
\item hidden units: 32, 64, 128;
\end{itemize}

\item[\textbf{Bag-of-Tricks}]: token embeddings representations are averaged into
a single vector representation, which is fed to a sigmoid classifier, we varied
the following parameters:

\begin{itemize}
\item n-grams: 2, 3, 4, 5;
\item top-k most frequent tokens: 100k, 90k, 80k;
\end{itemize}

\end{description}

Table~\ref{subtask_a_devset-results} shows the results for different classifiers
when trained with the best parameters on the training set and evaluated against
the development set. The Logistic Regression classifier achieved the best results
although the CNN classifier  had a similar F\textsubscript{1} score, essentially
by trading recall for precision.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Method}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Logit (TF-IDF) & 0.8211 & 0.8359 & 0.8284 \\
 CNN            & 0.8542 & 0.7879 & 0.8197 \\
 bi-LSTM        & 0.8062 & 0.7987 & 0.8024 \\
 Bag-of-Tricks  & 0.3787 & 0.6717 & 0.4843 \\
\hline
\end{tabular}
\end{center}
\caption{\label{subtask_a_devset-results} Results for different classifiers on the Sub-Task A on the development set.}
\end{table}

\subsubsection{Local classifier per node: Level 1 and 2}

For Level 1 and 2 in the hierarchy tree we trained a total of 50 classifiers, one
for each parent node, based on the CNN model by~\citet{kim-2014-convolutional}
using the variant of pre-trained embeddings which are fine-tuned during learning.

We explored some parameters configuration by varying the filter windows size and
the filter maps size, Table~\ref{subtask_b_parameters} shows a subset of the different
configuration parameters tried which yielded the best results. All this classifiers
 were training with mini-batch size of 16 for 5 epochs.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Configuration}  & \textbf{filter windows} &  \textbf{filter maps} \\
\hline
 Conf\_1 & 1,2       & 300 \\
 Conf\_2 & 1,2,3     & 200 \\
 Conf\_3 & 1,2,3,5,7 & 300 \\
 Conf\_4 & 3,5,6,10  & 256 \\
\hline
\end{tabular}
\end{center}
\caption{\label{subtask_b_parameters} Different configuration parameters for the CNN classifiers for Level 1 and 2.}
\end{table}

We then used the root level classifier which yielded the best results, i.e. the
logistic regression, to predict the labels for the root level, and experiment
with different parameters for the Levels 1 and 2. Table~\ref{level-1-2} shows
the results for Sub-Task B for a subset of the different configurations of parameters.

\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Method}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Conf\_1 & 0.7151 & 0.5330 & 0.6108 \\
 Conf\_2 & 0.7144 & 0.5303 & 0.6087 \\
 Conf\_3 & 0.7219 & 0.5235 & 0.6069 \\
 Conf\_4 & 0.7274 & 0.5085 & 0.5986 \\
\hline
\end{tabular}
\end{center}
\caption{\label{level-1-2} Results for Sub-Task B for different configurations
of the CNN-based classifier for Levels 1 and 2 of the hierarchy, using the best
classifier for the Root Level from Table~\ref{subtask_a_devset-results}.}
\end{table}






\subsubsection{Global classifier}

%TODO: results?

The global classifier uses a flattened hierarchy and tries to learn how to correctly
predict a vector of 343 dimensions given the training data. For this strategy we
used a CNN again based on the model proposed by~\cite{kim-2014-convolutional}.

Our initial idea with the global classifier was to explore to leverage on and
explore the label co-occurrence by initializing a weight matrix, but due to
time constrains this was not possible. Nevertheless, we explored different sets
configuration of parameters, varying again filter windows and the filter maps.


\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
Conf\_1 & & & \\
\hline
 Sub-Task A   &   &  &  \\
 Sub-Task B   &   &  &  \\
\hline
\hline
Conf\_2 & & & \\
\hline
 Sub-Task A   &   &  &  \\
 Sub-Task B   &   &  &  \\
\hline
\hline
Conf\_3 & & & \\
\hline
 Sub-Task A   &   &  &  \\
 Sub-Task B   &   &  &  \\
\hline
\hline
\end{tabular}
\end{center}
\caption{\label{}}
\end{table}



\begin{comment}
low	TRUE
simple	TRUE
stopwords	TRUE

[[256, 10], [256, 7], [256, 5], [256, 3], [256, 2], [256, 1]]
kernel_initializer=random_uniform
de-wiki-fasttext-300d-1M
dropout_p = 0.5
fully_connected_layers=2
adam
batch_size=128,
shuffle=True,
validation_split=0.4,
verbose=1,
epochs=250
filtered = np.array(len(labels2idx) * [0.4])


subtask_a			subtask_b
Precision	0.8389		Precision	0.6733
Recall	0.7659		Recall	0.5032
F1	0.8008		F1	0.576
\end{comment}



\begin{comment}

classifier			tokenisation schema
[[256, 10], [256, 7], [256, 5], [256, 3], [256, 2], [256, 1]]			low	TRUE
kernel_initializer=random_uniform			simple	TRUE
de-wiki-fasttext-300d-1M			stopwords	TRUE
dropout_p = 0.5
fully_connected_layers=2
adam
batch_size=128,
shuffle=True,
validation_split=0.4,
verbose=1,
epochs=500
filtered = np.array(len(labels2idx) * [0.4])

subtask_a			subtask_b
Precision	0.7875		Precision	0.6235
Recall	0.7942		Recall	0.5066
F1	0.7908		F1	0.559
\begin{comment}


\begin{comment}
[[256, 10], [256, 7], [256, 5], [256, 3], [256, 2], [256, 1]]
dropout_p = 0.5
fully_connected_layers=2
adam
batch_size=128,
shuffle=True,
epochs=250

subtask_a			subtask_b
Precision	0.3355		Precision	0.2381
Recall	0.6256		Recall	0.3057
F1	0.4368		F1	0.2677
\end{comment}





\subsection{Test Results}

We applied the classifiers described in the Section~\ref{experiments} with the
parameters that yielded the best results on the development dataset, by training
on all available data (i.e., training + development sets) and applied them
on the test dataset, therefore generating with two submissions for each Sub-Task.

\subsubsection{Parent Per Node: local\_clf\_logit\_cnn}

With the one parent per node strategy classifier, (\textit{local\_clf\_logit\_cnn})
we achieved the 13th best place on Sub-Task A, and the 11th best place on
Sub-Task B. Results for the different evaluation metrics are described in
Table~\ref{local_devset-results}.

% Comtravo-DS	Comtravo-DS__local_clf_logit_cnn
% subtask_a - Comtravo-DS	Comtravo-DS__local_clf_logit_cnn	13	0.7178	0.8144	0.8255	0.8199	0.943
% subtask_b - Comtravo-DS__local_clf_logit_cnn	11	0.1924	0.7042	0.5274	0.6031	1	0.943
\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Task}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Sub-Task A   &  0.8144 & 0.8255 & 0.8199 \\
 Sub-Task B   &  0.7042 & 0.5274 & 0.6031 \\
\hline
\end{tabular}
\end{center}
\caption{\label{local_devset-results} Best achieved results on the development
          set for both Sub-Tasks A and B with the parent per node classifier.}
\end{table}

Also, this classifier achieved the 9th recall and the 15th best precision
for Sub-Task A, and the 8th best recall and the 14th best precision for Sub-Task B.
This results were achieved with a support of 0.943, meaning that for
roughly 5\% of the samples the classifier did not managed to produce any
predictions.




\subsubsection{Global: global\_clf\_cnn}

With the global classifier strategy we achieved the 17th best place on Sub-Task A,
and the 13th best place on Sub-Task B. Detailed results are presented in
Table~\ref{global_devset-results}.

% Comtravo-DS__global_clf_cnn
% - Comtravo-DS__global_clf_cnn	17	0.7183	0.7761	0.7839	0.78390.7839	0.9925
% - Comtravo-DS__global_clf_cnn	13	0.1751	0.5672	0.5185	0.5418	0.9363	0.9986
\begin{table}[!h]
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline\centering\textbf{Task}  & \textbf{Precision} &  \textbf{Recall} &  \textbf{F\textsubscript{1}}\\
\hline
 Sub-Task A   &  0.7761 & 0.7839 & 0.7839 \\
 Sub-Task B   &  0.5672 & 0.5185 & 0.5418 \\
\hline
\end{tabular}
\end{center}
\caption{\label{global_devset-results} Best achieved results on the development
          set for both Sub-Tasks A and B with the global classifier.}
\end{table}

Also, this classifier achieved the 15th recall and the 17th best precision
for Sub-Task A, and the 9th best recall and the 18th best precision for Sub-Task B.
The support was 0.9986, meaning that for roughly 1\% of the samples the classifier
did not produced any predictions. The hierarchy consistency is of 0.9363, reflecting
the lack of the a proper post-processing step to enforce the hierarchical structure.

%TODO: are the results in-line with the dev test results?




\section{Future Work}\label{future}

%TODO:
% - Prediction threshold
% For the neural networks the padding was done to match the size of the longest document in the dataset.
% why not for the avg. number of tokens?

% Local classifier
A few more features could have been explored, for instance the author's name
and the release date of the book.

% Global Classifier
Due to time constrains we failed to employ a post-processing step on the global
classifier to make sure that the labels output is in-line with the hierarchy-tree
constrains.

The global classifier could be improved by initializing a weight matrix based on
label co-occurrence. \citet{kurata-etal-2016-improved} proposed a novel neural
network initialization method to treat some of the neurons in the final
hidden layer as dedicated neurons for each pattern of label co-occurrence
These dedicated neurons are initialized to connect to the corresponding
co-occurring labels with stronger weights than to others.

\bibliography{konvens2019.bib}
\bibliographystyle{acl_natbib}

\end{document}
